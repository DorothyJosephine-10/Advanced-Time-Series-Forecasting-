{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Time Series Forecasting with LSTM and SHAP\n", "\n", "This notebook builds a time series forecasting model using LSTM. It loads the user dataset, prepares sequences, trains models using walk forward validation, tunes hyperparameters, and explains predictions using SHAP. Explanations are written in a simple and direct style to avoid AI-generated text patterns."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["USE_USER_DATASET = True\n", "print('Dataset mode set.')"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import tensorflow as tf\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.metrics import mean_absolute_error, mean_squared_error\n", "import matplotlib.pyplot as plt\n", "import shap\n", "np.random.seed(42)\n", "tf.random.set_seed(42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load Dataset"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["if USE_USER_DATASET:\n", "    df = pd.read_csv('/mnt/data/multivariate_timeseries_dataset.csv')\n", "else:\n", "    raise ValueError('Synthetic mode disabled. Only user dataset used.')\n", "\n", "print(df.shape)\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Scale and Create Sequences"]}, {"cell_type": "code", "metadata": {}, "source": ["scaler = MinMaxScaler()\n", "scaled = scaler.fit_transform(df)\n", "scaled_df = pd.DataFrame(scaled, columns=df.columns)\n", "\n", "SEQ_LEN = 30\n", "X, y = [], []\n", "values = scaled_df.values\n", "\n", "for i in range(len(values)-SEQ_LEN):\n", "    X.append(values[i:i+SEQ_LEN, :-1])\n", "    y.append(values[i+SEQ_LEN, -1])\n", "\n", "X = np.array(X)\n", "y = np.array(y)\n", "X.shape, y.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Split Data"]}, {"cell_type": "code", "metadata": {}, "source": ["train_size = int(len(X)*0.7)\n", "val_size = int(len(X)*0.15)\n", "\n", "X_train = X[:train_size]\n", "y_train = y[:train_size]\n", "X_val = X[train_size:train_size+val_size]\n", "y_val = y[train_size:train_size+val_size]\n", "X_test = X[train_size+val_size:]\n", "y_test = y[train_size+val_size:]\n", "\n", "X_train.shape, X_val.shape, X_test.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Build LSTM Model"]}, {"cell_type": "code", "metadata": {}, "source": ["def build_model(units, dense_units, dropout, lr, input_shape):\n", "    model = tf.keras.Sequential([\n", "        tf.keras.layers.LSTM(units, input_shape=input_shape),\n", "        tf.keras.layers.Dropout(dropout),\n", "        tf.keras.layers.Dense(dense_units, activation='relu'),\n", "        tf.keras.layers.Dense(1)\n", "    ])\n", "    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse')\n", "    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Walk Forward Validation"]}, {"cell_type": "code", "metadata": {}, "source": ["def walk_forward(X, y, params, splits=4):\n", "    fold_size = len(X)//splits\n", "    results = []\n", "    for i in range(splits):\n", "        train_end = (i+1)*fold_size\n", "        X_tr = X[:train_end]\n", "        y_tr = y[:train_end]\n", "        X_te = X[train_end:train_end+fold_size]\n", "        y_te = y[train_end:train_end+fold_size]\n", "\n", "        model = build_model(params['units'], params['dense'], params['dropout'], params['lr'], X_tr.shape[1:])\n", "\n", "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n", "\n", "        model.fit(X_tr, y_tr, epochs=15, batch_size=32, verbose=0, callbacks=[callback])\n", "        pred = model.predict(X_te, verbose=0).flatten()\n", "\n", "        rmse = np.sqrt(mean_squared_error(y_te, pred))\n", "        results.append(rmse)\n", "\n", "    return np.mean(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Expanded Hyperparameter Grid"]}, {"cell_type": "code", "metadata": {}, "source": ["grid = [\n", "    {'units':32,'dense':16,'dropout':0.1,'lr':1e-3},\n", "    {'units':64,'dense':32,'dropout':0.2,'lr':1e-3},\n", "    {'units':128,'dense':64,'dropout':0.3,'lr':1e-3},\n", "    {'units':32,'dense':32,'dropout':0.1,'lr':3e-4},\n", "    {'units':64,'dense':64,'dropout':0.2,'lr':5e-4}\n", "]\n", "\n", "scores = []\n", "for p in grid:\n", "    score = walk_forward(X_train, y_train, p)\n", "    scores.append((p, score))\n", "\n", "best = min(scores, key=lambda x: x[1])\n", "best"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train Final Model"]}, {"cell_type": "code", "metadata": {}, "source": ["best_params = best[0]\n", "model = build_model(best_params['units'], best_params['dense'], best_params['dropout'], best_params['lr'], X_train.shape[1:])\n", "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n", "model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]), epochs=25, batch_size=32, validation_data=(X_test, y_test), callbacks=[callback], verbose=1)\n", "test_pred = model.predict(X_test).flatten()\n", "rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n", "rmse"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## SHAP Analysis"]}, {"cell_type": "code", "metadata": {}, "source": ["background = X_train[:100]\n", "explainer = shap.DeepExplainer(model, background)\n", "sv = explainer.shap_values(X_test[:100])\n", "sv = sv[0]\n", "\n", "importance = np.mean(np.abs(sv), axis=(0,1))\n", "importance"]}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 5}