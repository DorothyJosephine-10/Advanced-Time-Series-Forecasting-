{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Time Series Forecasting with Neural Networks and Explainable AI (XAI)\n",
    "\n",
    "This notebook implements an end‑to‑end deep learning pipeline for **multivariate time series forecasting** using an **LSTM** network and **Explainable AI (XAI)** with **SHAP**.\n",
    "\n",
    "It includes:\n",
    "- Programmatic generation of a complex multivariate time series dataset (≥ 1000 samples)\n",
    "- Data preprocessing (scaling, sequence creation, train/validation/test splits)\n",
    "- LSTM forecasting model with simple hyperparameter tuning\n",
    "- Walk‑forward cross‑validation with RMSE, MAE, and MAPE metrics\n",
    "- SHAP‑based feature importance analysis to interpret the trained model\n",
    "- Modular, well‑documented Python code (functions with docstrings & comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle between user dataset and synthetic data\n",
    "USE_USER_DATASET = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Global Configuration\n",
    "\n",
    "Run this cell first to install/import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "import shap\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using Your Provided Dataset Instead of Synthetic Data\n",
    "The following cell loads your uploaded dataset:\n",
    "`multivariate_timeseries_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('/mnt/data/multivariate_timeseries_dataset.csv')\n",
    "print('Loaded dataset shape:', df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Multivariate Time Series Data Generation\n",
    "\n",
    "We generate a **multivariate time series** that loosely mimics real‑world patterns such as stock/energy/sensor data:\n",
    "\n",
    "- `feature1`: noisy sine wave (e.g., cyclical demand)\n",
    "- `feature2`: noisy cosine wave (e.g., seasonal offset signal)\n",
    "- `feature3`: linear combination + noise (e.g., engineered feature)\n",
    "- `target`: weighted combination of features + additional noise (the value we want to forecast)\n",
    "\n",
    "This approach allows us to know there is a true relationship between features and target while keeping the dataset realistic and non‑trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_timeseries(n_samples: int = 2000) -> pd.DataFrame:\n",
    "    \"\"\"Generate a synthetic multivariate time series dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of time steps to generate (must be >= 1000 as per project spec).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns [feature1, feature2, feature3, target].\n",
    "    \"\"\"\n",
    "    time = np.arange(n_samples)\n",
    "\n",
    "    # Base signals with noise\n",
    "    feature1 = np.sin(0.02 * time) + np.random.normal(0, 0.15, n_samples)\n",
    "    feature2 = np.cos(0.02 * time) + np.random.normal(0, 0.15, n_samples)\n",
    "    feature3 = 0.5 * feature1 + 0.3 * feature2 + np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "    # Target depends on current & slightly lagged features (to make forecasting meaningful)\n",
    "    lag = np.roll(feature1, 1)\n",
    "    target = 0.4 * feature1 + 0.4 * feature2 + 0.15 * feature3 + 0.2 * lag\n",
    "    target += np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"feature1\": feature1,\n",
    "        \"feature2\": feature2,\n",
    "        \"feature3\": feature3,\n",
    "        \"target\": target,\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Generate and inspect the dataset\n",
    "df = generate_synthetic_timeseries(n_samples=2000)\n",
    "display(df.head())\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Quick visualization of raw series\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df[\"target\"].values, label=\"target\")\n",
    "plt.title(\"Synthetic Target Time Series\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Sequence Creation\n",
    "\n",
    "We now:\n",
    "\n",
    "1. **Scale** all features to `[0, 1]` using MinMaxScaler.\n",
    "2. Convert the time series into **supervised learning sequences** for the LSTM.\n",
    "   - Given a **window** of `seq_len` past time steps of all features, the model predicts the **next target value**.\n",
    "3. Split the data into **train**, **validation**, and **test** segments while preserving temporal order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataframe(df: pd.DataFrame) -> Tuple[pd.DataFrame, MinMaxScaler]:\n",
    "    \"\"\"Scale all columns of the DataFrame to [0, 1].\n",
    "\n",
    "    Returns the scaled DataFrame and the fitted scaler.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_values = scaler.fit_transform(df.values)\n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=df.columns, index=df.index)\n",
    "    return scaled_df, scaler\n",
    "\n",
    "\n",
    "def create_sequences(\n",
    "    df: pd.DataFrame,\n",
    "    seq_len: int = 30,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create input/output sequences for LSTM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Scaled multivariate time series including the target column.\n",
    "    seq_len : int\n",
    "        Number of past time steps used as input.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Input tensor with shape (n_samples - seq_len, seq_len, n_features-1).\n",
    "    y : np.ndarray\n",
    "        Target vector with shape (n_samples - seq_len, ).\n",
    "    \"\"\"\n",
    "    values = df.values\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(values) - seq_len):\n",
    "        # Use all feature columns except the last (target)\n",
    "        X.append(values[i : i + seq_len, :-1])\n",
    "        # Predict the target at the next time step\n",
    "        y.append(values[i + seq_len, -1])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def train_val_test_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    ") -> Tuple[np.ndarray, ...]:\n",
    "    \"\"\"Split sequences into train/validation/test sets while preserving order.\"\"\"\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "\n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "    X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "scaled_df, scaler = scale_dataframe(df)\n",
    "SEQ_LEN = 30\n",
    "X, y = create_sequences(scaled_df, seq_len=SEQ_LEN)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM Model Definition\n",
    "\n",
    "We create a configurable **LSTM** model. Hyperparameters such as number of units, dropout, and dense units can be tuned later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(\n",
    "    input_shape: Tuple[int, int],\n",
    "    lstm_units: int = 64,\n",
    "    dense_units: int = 32,\n",
    "    dropout_rate: float = 0.1,\n",
    "    learning_rate: float = 1e-3,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"Build and compile an LSTM forecasting model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : tuple\n",
    "        (seq_len, n_features)\n",
    "    lstm_units : int\n",
    "        Number of LSTM units.\n",
    "    dense_units : int\n",
    "        Units in the hidden dense layer.\n",
    "    dropout_rate : float\n",
    "        Dropout rate after the LSTM layer.\n",
    "    learning_rate : float\n",
    "        Learning rate for Adam optimizer.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, input_shape=input_shape, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation=\"relu\"),\n",
    "        Dense(1),  # Forecast next target value\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute RMSE, MAE, and MAPE between true and predicted values.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"mape\": mape}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Walk‑Forward Cross‑Validation & Hyperparameter Tuning\n",
    "\n",
    "To evaluate robustness over time, we use **walk‑forward validation**:\n",
    "\n",
    "1. Split the training+validation portion of the data into `k` folds over time.\n",
    "2. For each fold, train on all data *up to* that fold and test on the next chunk.\n",
    "3. Aggregate metrics across folds.\n",
    "\n",
    "We repeat this process for a small grid of hyperparameters and choose the configuration with the best average RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    n_splits: int,\n",
    "    build_model_fn,\n",
    "    epochs: int = 15,\n",
    "    batch_size: int = 32,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Perform walk‑forward cross‑validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, y : np.ndarray\n",
    "        Input sequences and corresponding targets.\n",
    "    n_splits : int\n",
    "        Number of temporal folds.\n",
    "    build_model_fn : callable\n",
    "        Function that returns a compiled Keras model.\n",
    "    epochs : int\n",
    "        Number of epochs for each fold.\n",
    "    batch_size : int\n",
    "        Training batch size.\n",
    "    \"\"\"\n",
    "    fold_size = len(X) // n_splits\n",
    "    metrics_list: List[Dict[str, float]] = []\n",
    "\n",
    "    for fold in range(n_splits):\n",
    "        print(f\"\\n▶ Fold {fold + 1}/{n_splits}\")\n",
    "        end_train = (fold + 1) * fold_size\n",
    "        start_test = end_train\n",
    "        end_test = start_test + fold_size\n",
    "\n",
    "        if start_test >= len(X):\n",
    "            break\n",
    "\n",
    "        X_train_fold = X[:end_train]\n",
    "        y_train_fold = y[:end_train]\n",
    "        X_test_fold = X[start_test:end_test]\n",
    "        y_test_fold = y[start_test:end_test]\n",
    "\n",
    "        model = build_model_fn()\n",
    "        history = model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        y_pred_fold = model.predict(X_test_fold, verbose=0).flatten()\n",
    "        metrics = compute_regression_metrics(y_test_fold, y_pred_fold)\n",
    "        metrics_list.append(metrics)\n",
    "        print(f\"Fold {fold+1} metrics: RMSE={metrics['rmse']:.4f}, MAE={metrics['mae']:.4f}, MAPE={metrics['mape']:.2f}%\")\n",
    "\n",
    "    # Aggregate metrics\n",
    "    agg = {\n",
    "        \"rmse\": np.mean([m[\"rmse\"] for m in metrics_list]),\n",
    "        \"mae\": np.mean([m[\"mae\"] for m in metrics_list]),\n",
    "        \"mape\": np.mean([m[\"mape\"] for m in metrics_list]),\n",
    "    }\n",
    "    print(\"\\nAverage cross‑validation metrics:\")\n",
    "    print(\"RMSE={:.4f}, MAE={:.4f}, MAPE={:.2f}%\".format(agg[\"rmse\"], agg[\"mae\"], agg[\"mape\"]))\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "# === Hyperparameter search over a small grid ===\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "param_grid = [\n",
    "    {\"lstm_units\": 32, \"dense_units\": 16, \"dropout\": 0.1, \"lr\": 1e-3},\n",
    "    {\"lstm_units\": 64, \"dense_units\": 32, \"dropout\": 0.2, \"lr\": 1e-3},\n",
    "    {\"lstm_units\": 64, \"dense_units\": 64, \"dropout\": 0.3, \"lr\": 5e-4},\n",
    "]\n",
    "\n",
    "cv_results = []\n",
    "N_SPLITS = 4\n",
    "\n",
    "for i, params in enumerate(param_grid):\n",
    "    print(\"\\n============================\")\n",
    "    print(f\"Hyperparameter set {i+1}/{len(param_grid)}: {params}\")\n",
    "\n",
    "    def model_builder():\n",
    "        return build_lstm_model(\n",
    "            input_shape=input_shape,\n",
    "            lstm_units=params[\"lstm_units\"],\n",
    "            dense_units=params[\"dense_units\"],\n",
    "            dropout_rate=params[\"dropout\"],\n",
    "            learning_rate=params[\"lr\"],\n",
    "        )\n",
    "\n",
    "    metrics = walk_forward_validation(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        n_splits=N_SPLITS,\n",
    "        build_model_fn=model_builder,\n",
    "        epochs=12,\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "    cv_results.append({\"params\": params, \"metrics\": metrics})\n",
    "\n",
    "# Select best hyperparameters based on RMSE\n",
    "best_result = min(cv_results, key=lambda r: r[\"metrics\"][\"rmse\"])\n",
    "best_params = best_result[\"params\"]\n",
    "print(\"\\nBest hyperparameters based on CV RMSE:\")\n",
    "print(best_params)\n",
    "print(\"Corresponding metrics:\", best_result[\"metrics\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Final Model & Evaluate on Test Set\n",
    "\n",
    "We now train a final LSTM model using the **best hyperparameters** found above on the combined **train + validation** data and evaluate it on the **unseen test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train + validation for final training\n",
    "X_final_train = np.concatenate([X_train, X_val], axis=0)\n",
    "y_final_train = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "final_model = build_lstm_model(\n",
    "    input_shape=input_shape,\n",
    "    lstm_units=best_params[\"lstm_units\"],\n",
    "    dense_units=best_params[\"dense_units\"],\n",
    "    dropout_rate=best_params[\"dropout\"],\n",
    "    learning_rate=best_params[\"lr\"],\n",
    ")\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_final_train,\n",
    "    y_final_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = final_model.predict(X_test).flatten()\n",
    "test_metrics = compute_regression_metrics(y_test, y_test_pred)\n",
    "print(\"\\nTest set metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k.upper()}: {v:.4f}\")\n",
    "\n",
    "# Plot predictions vs actuals on a subset of the test set\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(y_test[:200], label=\"True\")\n",
    "plt.plot(y_test_pred[:200], label=\"Predicted\", linestyle=\"--\")\n",
    "plt.title(\"LSTM Forecast vs True Target (Test Subset)\")\n",
    "plt.xlabel(\"Time step (relative in test set)\")\n",
    "plt.ylabel(\"Scaled target value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explainable AI (XAI) with SHAP\n",
    "\n",
    "We apply **SHAP (SHapley Additive exPlanations)** to interpret the trained LSTM model.\n",
    "\n",
    "Steps:\n",
    "1. Choose a **background dataset** (a small sample from the training data).\n",
    "2. Use `shap.DeepExplainer` to compute SHAP values for a subset of sequences.\n",
    "3. Aggregate absolute SHAP values over time steps to estimate the **global importance of each feature**.\n",
    "\n",
    "Note: DeepExplainer works best with certain TensorFlow/Keras versions. If you encounter issues, consider installing a compatible SHAP version or using KernelExplainer on a flattened representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small background sample for SHAP (to keep computation reasonable)\n",
    "background_size = min(200, len(X_final_train))\n",
    "background = X_final_train[:background_size]\n",
    "\n",
    "# Select instances to explain from the test set\n",
    "explain_size = min(200, len(X_test))\n",
    "to_explain = X_test[:explain_size]\n",
    "\n",
    "print(\"Computing SHAP values (this may take a few minutes)...\")\n",
    "explainer = shap.DeepExplainer(final_model, background)\n",
    "shap_values = explainer.shap_values(to_explain)\n",
    "\n",
    "# shap_values is a list with one array (regression output)\n",
    "shap_values_arr = shap_values[0]  # shape: (samples, seq_len, n_features)\n",
    "print(\"SHAP values shape:\", shap_values_arr.shape)\n",
    "\n",
    "# Aggregate absolute SHAP values over time and samples to get global feature importance\n",
    "abs_shap = np.abs(shap_values_arr)\n",
    "feature_importance = abs_shap.mean(axis=(0, 1))  # mean over samples and time\n",
    "\n",
    "feature_names = df.columns[:-1]  # exclude target\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"mean_abs_shap\": feature_importance,\n",
    "}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "print(\"\\nGlobal feature importance based on SHAP:\")\n",
    "display(importance_df)\n",
    "\n",
    "# Optional: SHAP summary plot (uncomment to visualize)\n",
    "# shap.summary_plot(shap_values_arr, features=to_explain, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Summary of Results & XAI Insights\n",
    "\n",
    "The following cell prints a concise, text‑based report describing:\n",
    "\n",
    "- Final model architecture and best hyperparameters\n",
    "- Walk‑forward cross‑validation metrics\n",
    "- Test‑set performance\n",
    "- SHAP‑derived feature importance ranking\n",
    "\n",
    "You can copy this section directly into your project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_report(\n",
    "    best_params: Dict,\n",
    "    cv_results: List[Dict],\n",
    "    test_metrics: Dict[str, float],\n",
    "    importance_df: pd.DataFrame,\n",
    ") -> str:\n",
    "    \"\"\"Generate a human‑readable summary of experiment results.\"\"\"\n",
    "    avg_rmse = best_result[\"metrics\"][\"rmse\"]\n",
    "    avg_mae = best_result[\"metrics\"][\"mae\"]\n",
    "    avg_mape = best_result[\"metrics\"][\"mape\"]\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"MODEL & TRAINING SUMMARY\")\n",
    "    lines.append(\"- Final model: Single‑layer LSTM followed by a dense hidden layer and linear output.\")\n",
    "    lines.append(\n",
    "        f\"- Best hyperparameters (from walk‑forward CV): LSTM units={best_params['lstm_units']}, \"\n",
    "        f\"Dense units={best_params['dense_units']}, Dropout={best_params['dropout']}, \"\n",
    "        f\"Learning rate={best_params['lr']}.\"\n",
    "    )\n",
    "    lines.append(\n",
    "        f\"- Average walk‑forward validation performance: RMSE={avg_rmse:.4f}, MAE={avg_mae:.4f}, MAPE={avg_mape:.2f}% .\"\n",
    "    )\n",
    "    lines.append(\n",
    "        f\"- Held‑out test set performance: RMSE={test_metrics['rmse']:.4f}, MAE={test_metrics['mae']:.4f}, MAPE={test_metrics['mape']:.2f}% .\"\n",
    "    )\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"XAI / FEATURE IMPORTANCE INSIGHTS (SHAP)\")\n",
    "    lines.append(\"- Global importance ranking (most to least influential features):\")\n",
    "\n",
    "    for i, row in importance_df.iterrows():\n",
    "        lines.append(\n",
    "            f\"  {row['feature']}: mean |SHAP| = {row['mean_abs_shap']:.6f}\"\n",
    "        )\n",
    "\n",
    "    lines.append(\n",
    "        \"- Features with larger mean |SHAP| values have stronger influence on the model's \"\n",
    "        \"predictions across time.\"\n",
    "    )\n",
    "\n",
    "    report = \"\\n\".join(lines)\n",
    "    return report\n",
    "\n",
    "\n",
    "report_text = generate_text_report(best_params, cv_results, test_metrics, importance_df)\n",
    "print(report_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How to Use This Notebook in Your Project\n",
    "\n",
    "- Treat Sections **2–7** as the full implementation of the pipeline.\n",
    "- Use Section **8** as the basis for your written report.\n",
    "- You can further extend this notebook by:\n",
    "  - Trying different model architectures (e.g., stacked LSTMs, TCNs)\n",
    "  - Adding early stopping / learning‑rate schedulers\n",
    "  - Performing more extensive hyperparameter tuning\n",
    "  - Exporting the trained model with `model.save()` for deployment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
